{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl import config_tickers\n",
    "from finrl.config import INDICATORS\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2009-01-01'\n",
    "TRAIN_END_DATE = '2020-07-01'\n",
    "TRADE_START_DATE = '2020-07-01'\n",
    "TRADE_END_DATE = '2022-12-01'\n",
    "\n",
    "symbols = [\n",
    "    'aapl',\n",
    "    'msft',\n",
    "    'meta',\n",
    "    'ibm',\n",
    "    'hd',\n",
    "    'cat',\n",
    "    'amzn',\n",
    "    'intc',\n",
    "    't',\n",
    "    'v'\n",
    "    'gs'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (32040, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                                end_date = TRADE_END_DATE,\n",
    "                                ticker_list = symbols).fetch_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "         date       open       high        low      close     volume   tic  \\\n0  2009-01-02   3.067143   3.251429   3.041429   2.747390  746015200  aapl   \n1  2009-01-02   2.567500   2.726500   2.553500   2.718000  145928000  amzn   \n2  2009-01-02  44.910000  46.980000  44.709999  30.950012    7117200   cat   \n3  2009-01-02  23.070000  24.190001  22.959999  16.649143   14902500    hd   \n4  2009-01-02  80.200768  83.738052  80.200768  50.100773    7905877   ibm   \n\n   day  \n0    4  \n1    4  \n2    4  \n3    4  \n4    4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>tic</th>\n      <th>day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>3.067143</td>\n      <td>3.251429</td>\n      <td>3.041429</td>\n      <td>2.747390</td>\n      <td>746015200</td>\n      <td>aapl</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-01-02</td>\n      <td>2.567500</td>\n      <td>2.726500</td>\n      <td>2.553500</td>\n      <td>2.718000</td>\n      <td>145928000</td>\n      <td>amzn</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-01-02</td>\n      <td>44.910000</td>\n      <td>46.980000</td>\n      <td>44.709999</td>\n      <td>30.950012</td>\n      <td>7117200</td>\n      <td>cat</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-01-02</td>\n      <td>23.070000</td>\n      <td>24.190001</td>\n      <td>22.959999</td>\n      <td>16.649143</td>\n      <td>14902500</td>\n      <td>hd</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-01-02</td>\n      <td>80.200768</td>\n      <td>83.738052</td>\n      <td>80.200768</td>\n      <td>50.100773</td>\n      <td>7905877</td>\n      <td>ibm</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3502, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_vix=True,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df_raw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "             date        open        high         low       close     volume  \\\n0      2009-01-02    3.067143    3.251429    3.041429    2.747390  746015200   \n1      2009-01-02    2.567500    2.726500    2.553500    2.718000  145928000   \n2      2009-01-02   44.910000   46.980000   44.709999   30.950012    7117200   \n3      2009-01-02   23.070000   24.190001   22.959999   16.649143   14902500   \n4      2009-01-02   80.200768   83.738052   80.200768   50.100773    7905877   \n...           ...         ...         ...         ...         ...        ...   \n28011  2022-11-29  316.000000  320.000000  315.619995  305.557312    3505600   \n28012  2022-11-29  145.910004  147.169998  145.699997  139.558807    2754700   \n28013  2022-11-29   28.850000   29.180000   28.740000   28.240854   24361500   \n28014  2022-11-29  241.399994  242.789993  238.210007  238.217743   17956300   \n28015  2022-11-29   18.790001   19.030001   18.750000   17.527452   24088000   \n\n        tic  day      macd     boll_ub     boll_lb      rsi_30     cci_30  \\\n0      aapl    4  0.000000    2.969346    2.641385  100.000000  66.666667   \n1      amzn    4  0.000000    2.969346    2.641385  100.000000  66.666667   \n2       cat    4  0.000000    2.969346    2.641385  100.000000  66.666667   \n3        hd    4  0.000000    2.969346    2.641385  100.000000  66.666667   \n4       ibm    4  0.000000    2.969346    2.641385  100.000000  66.666667   \n...     ...  ...       ...         ...         ...         ...        ...   \n28011    hd    1  8.491188  324.439971  268.177372   56.444578  83.144966   \n28012   ibm    1  4.523366  145.729322  125.716428   63.696291  83.912091   \n28013  intc    1  0.435235   30.506556   26.224579   49.010665  33.364885   \n28014  msft    1  2.267116  254.737121  213.164572   49.156757  34.946232   \n28015     t    1  0.446189   17.881416   16.719863   60.467981  64.832371   \n\n            dx_30  close_30_sma  close_60_sma        vix  turbulence  \n0      100.000000      2.747390      2.747390  39.189999    0.000000  \n1      100.000000      2.718000      2.718000  39.189999    0.000000  \n2      100.000000     30.950012     30.950012  39.189999    0.000000  \n3      100.000000     16.649143     16.649143  39.189999    0.000000  \n4      100.000000     50.100773     50.100773  39.189999    0.000000  \n...           ...           ...           ...        ...         ...  \n28011   22.103961    289.574117    280.954247  21.889999    3.949013  \n28012   42.080709    131.775917    124.174526  21.889999    3.949013  \n28013    0.768171     27.603657     27.243324  21.889999    3.949013  \n28014    0.428685    234.291056    237.173987  21.889999    3.949013  \n28015   20.258599     16.852541     15.702813  21.889999    3.949013  \n\n[28016 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>tic</th>\n      <th>day</th>\n      <th>macd</th>\n      <th>boll_ub</th>\n      <th>boll_lb</th>\n      <th>rsi_30</th>\n      <th>cci_30</th>\n      <th>dx_30</th>\n      <th>close_30_sma</th>\n      <th>close_60_sma</th>\n      <th>vix</th>\n      <th>turbulence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>3.067143</td>\n      <td>3.251429</td>\n      <td>3.041429</td>\n      <td>2.747390</td>\n      <td>746015200</td>\n      <td>aapl</td>\n      <td>4</td>\n      <td>0.000000</td>\n      <td>2.969346</td>\n      <td>2.641385</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>2.747390</td>\n      <td>2.747390</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-01-02</td>\n      <td>2.567500</td>\n      <td>2.726500</td>\n      <td>2.553500</td>\n      <td>2.718000</td>\n      <td>145928000</td>\n      <td>amzn</td>\n      <td>4</td>\n      <td>0.000000</td>\n      <td>2.969346</td>\n      <td>2.641385</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>2.718000</td>\n      <td>2.718000</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-01-02</td>\n      <td>44.910000</td>\n      <td>46.980000</td>\n      <td>44.709999</td>\n      <td>30.950012</td>\n      <td>7117200</td>\n      <td>cat</td>\n      <td>4</td>\n      <td>0.000000</td>\n      <td>2.969346</td>\n      <td>2.641385</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>30.950012</td>\n      <td>30.950012</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-01-02</td>\n      <td>23.070000</td>\n      <td>24.190001</td>\n      <td>22.959999</td>\n      <td>16.649143</td>\n      <td>14902500</td>\n      <td>hd</td>\n      <td>4</td>\n      <td>0.000000</td>\n      <td>2.969346</td>\n      <td>2.641385</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>16.649143</td>\n      <td>16.649143</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-01-02</td>\n      <td>80.200768</td>\n      <td>83.738052</td>\n      <td>80.200768</td>\n      <td>50.100773</td>\n      <td>7905877</td>\n      <td>ibm</td>\n      <td>4</td>\n      <td>0.000000</td>\n      <td>2.969346</td>\n      <td>2.641385</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>50.100773</td>\n      <td>50.100773</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>28011</th>\n      <td>2022-11-29</td>\n      <td>316.000000</td>\n      <td>320.000000</td>\n      <td>315.619995</td>\n      <td>305.557312</td>\n      <td>3505600</td>\n      <td>hd</td>\n      <td>1</td>\n      <td>8.491188</td>\n      <td>324.439971</td>\n      <td>268.177372</td>\n      <td>56.444578</td>\n      <td>83.144966</td>\n      <td>22.103961</td>\n      <td>289.574117</td>\n      <td>280.954247</td>\n      <td>21.889999</td>\n      <td>3.949013</td>\n    </tr>\n    <tr>\n      <th>28012</th>\n      <td>2022-11-29</td>\n      <td>145.910004</td>\n      <td>147.169998</td>\n      <td>145.699997</td>\n      <td>139.558807</td>\n      <td>2754700</td>\n      <td>ibm</td>\n      <td>1</td>\n      <td>4.523366</td>\n      <td>145.729322</td>\n      <td>125.716428</td>\n      <td>63.696291</td>\n      <td>83.912091</td>\n      <td>42.080709</td>\n      <td>131.775917</td>\n      <td>124.174526</td>\n      <td>21.889999</td>\n      <td>3.949013</td>\n    </tr>\n    <tr>\n      <th>28013</th>\n      <td>2022-11-29</td>\n      <td>28.850000</td>\n      <td>29.180000</td>\n      <td>28.740000</td>\n      <td>28.240854</td>\n      <td>24361500</td>\n      <td>intc</td>\n      <td>1</td>\n      <td>0.435235</td>\n      <td>30.506556</td>\n      <td>26.224579</td>\n      <td>49.010665</td>\n      <td>33.364885</td>\n      <td>0.768171</td>\n      <td>27.603657</td>\n      <td>27.243324</td>\n      <td>21.889999</td>\n      <td>3.949013</td>\n    </tr>\n    <tr>\n      <th>28014</th>\n      <td>2022-11-29</td>\n      <td>241.399994</td>\n      <td>242.789993</td>\n      <td>238.210007</td>\n      <td>238.217743</td>\n      <td>17956300</td>\n      <td>msft</td>\n      <td>1</td>\n      <td>2.267116</td>\n      <td>254.737121</td>\n      <td>213.164572</td>\n      <td>49.156757</td>\n      <td>34.946232</td>\n      <td>0.428685</td>\n      <td>234.291056</td>\n      <td>237.173987</td>\n      <td>21.889999</td>\n      <td>3.949013</td>\n    </tr>\n    <tr>\n      <th>28015</th>\n      <td>2022-11-29</td>\n      <td>18.790001</td>\n      <td>19.030001</td>\n      <td>18.750000</td>\n      <td>17.527452</td>\n      <td>24088000</td>\n      <td>t</td>\n      <td>1</td>\n      <td>0.446189</td>\n      <td>17.881416</td>\n      <td>16.719863</td>\n      <td>60.467981</td>\n      <td>64.832371</td>\n      <td>20.258599</td>\n      <td>16.852541</td>\n      <td>15.702813</td>\n      <td>21.889999</td>\n      <td>3.949013</td>\n    </tr>\n  </tbody>\n</table>\n<p>28016 rows × 18 columns</p>\n</div>"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "processed_full = processed_full.sort_values(['date','tic'])\n",
    "\n",
    "processed_full = processed_full.fillna(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "             date   tic        open        high         low       close  \\\n0      2009-01-02  aapl    3.067143    3.251429    3.041429    2.747390   \n1      2009-01-02  amzn    2.567500    2.726500    2.553500    2.718000   \n2      2009-01-02   cat   44.910000   46.980000   44.709999   30.950012   \n3      2009-01-02    hd   23.070000   24.190001   22.959999   16.649143   \n4      2009-01-02   ibm   80.200768   83.738052   80.200768   50.100773   \n...           ...   ...         ...         ...         ...         ...   \n40635  2022-11-29    hd  316.000000  320.000000  315.619995  305.557312   \n40636  2022-11-29   ibm  145.910004  147.169998  145.699997  139.558807   \n40637  2022-11-29  intc   28.850000   29.180000   28.740000   28.240854   \n40638  2022-11-29  msft  241.399994  242.789993  238.210007  238.217743   \n40639  2022-11-29     t   18.790001   19.030001   18.750000   17.527452   \n\n            volume  day      macd     boll_ub     boll_lb      rsi_30  \\\n0      746015200.0  4.0  0.000000    2.969346    2.641385  100.000000   \n1      145928000.0  4.0  0.000000    2.969346    2.641385  100.000000   \n2        7117200.0  4.0  0.000000    2.969346    2.641385  100.000000   \n3       14902500.0  4.0  0.000000    2.969346    2.641385  100.000000   \n4        7905877.0  4.0  0.000000    2.969346    2.641385  100.000000   \n...            ...  ...       ...         ...         ...         ...   \n40635    3505600.0  1.0  8.491188  324.439971  268.177372   56.444578   \n40636    2754700.0  1.0  4.523366  145.729322  125.716428   63.696291   \n40637   24361500.0  1.0  0.435235   30.506556   26.224579   49.010665   \n40638   17956300.0  1.0  2.267116  254.737121  213.164572   49.156757   \n40639   24088000.0  1.0  0.446189   17.881416   16.719863   60.467981   \n\n          cci_30       dx_30  close_30_sma  close_60_sma        vix  \\\n0      66.666667  100.000000      2.747390      2.747390  39.189999   \n1      66.666667  100.000000      2.718000      2.718000  39.189999   \n2      66.666667  100.000000     30.950012     30.950012  39.189999   \n3      66.666667  100.000000     16.649143     16.649143  39.189999   \n4      66.666667  100.000000     50.100773     50.100773  39.189999   \n...          ...         ...           ...           ...        ...   \n40635  83.144966   22.103961    289.574117    280.954247  21.889999   \n40636  83.912091   42.080709    131.775917    124.174526  21.889999   \n40637  33.364885    0.768171     27.603657     27.243324  21.889999   \n40638  34.946232    0.428685    234.291056    237.173987  21.889999   \n40639  64.832371   20.258599     16.852541     15.702813  21.889999   \n\n       turbulence  \n0        0.000000  \n1        0.000000  \n2        0.000000  \n3        0.000000  \n4        0.000000  \n...           ...  \n40635    3.949013  \n40636    3.949013  \n40637    3.949013  \n40638    3.949013  \n40639    3.949013  \n\n[28016 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>tic</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>day</th>\n      <th>macd</th>\n      <th>boll_ub</th>\n      <th>boll_lb</th>\n      <th>rsi_30</th>\n      <th>cci_30</th>\n      <th>dx_30</th>\n      <th>close_30_sma</th>\n      <th>close_60_sma</th>\n      <th>vix</th>\n      <th>turbulence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>aapl</td>\n      <td>3.067143</td>\n      <td>3.251429</td>\n      <td>3.041429</td>\n      <td>2.747390</td>\n      <td>746015200.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.969346</td>\n      <td>2.641385</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>2.747390</td>\n      <td>2.747390</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-01-02</td>\n      <td>amzn</td>\n      <td>2.567500</td>\n      <td>2.726500</td>\n      <td>2.553500</td>\n      <td>2.718000</td>\n      <td>145928000.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.969346</td>\n      <td>2.641385</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>2.718000</td>\n      <td>2.718000</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-01-02</td>\n      <td>cat</td>\n      <td>44.910000</td>\n      <td>46.980000</td>\n      <td>44.709999</td>\n      <td>30.950012</td>\n      <td>7117200.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.969346</td>\n      <td>2.641385</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>30.950012</td>\n      <td>30.950012</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-01-02</td>\n      <td>hd</td>\n      <td>23.070000</td>\n      <td>24.190001</td>\n      <td>22.959999</td>\n      <td>16.649143</td>\n      <td>14902500.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.969346</td>\n      <td>2.641385</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>16.649143</td>\n      <td>16.649143</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-01-02</td>\n      <td>ibm</td>\n      <td>80.200768</td>\n      <td>83.738052</td>\n      <td>80.200768</td>\n      <td>50.100773</td>\n      <td>7905877.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.969346</td>\n      <td>2.641385</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>50.100773</td>\n      <td>50.100773</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>40635</th>\n      <td>2022-11-29</td>\n      <td>hd</td>\n      <td>316.000000</td>\n      <td>320.000000</td>\n      <td>315.619995</td>\n      <td>305.557312</td>\n      <td>3505600.0</td>\n      <td>1.0</td>\n      <td>8.491188</td>\n      <td>324.439971</td>\n      <td>268.177372</td>\n      <td>56.444578</td>\n      <td>83.144966</td>\n      <td>22.103961</td>\n      <td>289.574117</td>\n      <td>280.954247</td>\n      <td>21.889999</td>\n      <td>3.949013</td>\n    </tr>\n    <tr>\n      <th>40636</th>\n      <td>2022-11-29</td>\n      <td>ibm</td>\n      <td>145.910004</td>\n      <td>147.169998</td>\n      <td>145.699997</td>\n      <td>139.558807</td>\n      <td>2754700.0</td>\n      <td>1.0</td>\n      <td>4.523366</td>\n      <td>145.729322</td>\n      <td>125.716428</td>\n      <td>63.696291</td>\n      <td>83.912091</td>\n      <td>42.080709</td>\n      <td>131.775917</td>\n      <td>124.174526</td>\n      <td>21.889999</td>\n      <td>3.949013</td>\n    </tr>\n    <tr>\n      <th>40637</th>\n      <td>2022-11-29</td>\n      <td>intc</td>\n      <td>28.850000</td>\n      <td>29.180000</td>\n      <td>28.740000</td>\n      <td>28.240854</td>\n      <td>24361500.0</td>\n      <td>1.0</td>\n      <td>0.435235</td>\n      <td>30.506556</td>\n      <td>26.224579</td>\n      <td>49.010665</td>\n      <td>33.364885</td>\n      <td>0.768171</td>\n      <td>27.603657</td>\n      <td>27.243324</td>\n      <td>21.889999</td>\n      <td>3.949013</td>\n    </tr>\n    <tr>\n      <th>40638</th>\n      <td>2022-11-29</td>\n      <td>msft</td>\n      <td>241.399994</td>\n      <td>242.789993</td>\n      <td>238.210007</td>\n      <td>238.217743</td>\n      <td>17956300.0</td>\n      <td>1.0</td>\n      <td>2.267116</td>\n      <td>254.737121</td>\n      <td>213.164572</td>\n      <td>49.156757</td>\n      <td>34.946232</td>\n      <td>0.428685</td>\n      <td>234.291056</td>\n      <td>237.173987</td>\n      <td>21.889999</td>\n      <td>3.949013</td>\n    </tr>\n    <tr>\n      <th>40639</th>\n      <td>2022-11-29</td>\n      <td>t</td>\n      <td>18.790001</td>\n      <td>19.030001</td>\n      <td>18.750000</td>\n      <td>17.527452</td>\n      <td>24088000.0</td>\n      <td>1.0</td>\n      <td>0.446189</td>\n      <td>17.881416</td>\n      <td>16.719863</td>\n      <td>60.467981</td>\n      <td>64.832371</td>\n      <td>20.258599</td>\n      <td>16.852541</td>\n      <td>15.702813</td>\n      <td>21.889999</td>\n      <td>3.949013</td>\n    </tr>\n  </tbody>\n</table>\n<p>28016 rows × 18 columns</p>\n</div>"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_full"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23144\n",
      "4872\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "train = data_split(processed_full, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "train_path = data_path + 'candles/train.csv'\n",
    "trade_path = data_path + 'candles/trade.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "with open(train_path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  train.to_csv(f)\n",
    "  f.close()\n",
    "with open(trade_path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  trade.to_csv(f)\n",
    "  f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "######################################################################\n",
    "##############################   SECTION2   ##########################\n",
    "######################################################################\n",
    "\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl import config_tickers\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 8, State Space: 81\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 64}\n",
      "Using cpu device\n",
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n",
      "Logging to results/ddpg\n",
      "Logging to results/ppo\n",
      "Logging to results/td3\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "source": [
    "# A2C Agent\n",
    "\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "model_ddpg = agent.get_model('ddpg')\n",
    "model_ppo = agent.get_model('ppo')\n",
    "model_td3 = agent.get_model('td3')\n",
    "model_sac = agent.get_model('sac')\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ddpg)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_td3)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_sac)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 243         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -11.5       |\n",
      "|    explained_variance | -0.0161     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 14.8        |\n",
      "|    reward             | -0.68778497 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.85        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 244        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 10.9       |\n",
      "|    reward             | -3.2392704 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.29       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 242      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -11.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -109     |\n",
      "|    reward             | 2.296795 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 102      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 248         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -11.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -18.1       |\n",
      "|    reward             | -0.44324163 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 3.98        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 250        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.5      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 183        |\n",
      "|    reward             | -15.591586 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 398        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 251       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 27.4      |\n",
      "|    reward             | 0.2330289 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.69      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 250       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -65.1     |\n",
      "|    reward             | -8.918382 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 31.8      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 249          |\n",
      "|    iterations         | 800          |\n",
      "|    time_elapsed       | 16           |\n",
      "|    total_timesteps    | 4000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -11.4        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 799          |\n",
      "|    policy_loss        | 33.4         |\n",
      "|    reward             | -0.044470597 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 9.96         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 250        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 19         |\n",
      "|    reward             | -1.2883544 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.19       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 251       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 53.7      |\n",
      "|    reward             | -8.759567 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 20.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 251       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -192      |\n",
      "|    reward             | 7.4325542 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 363       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 253       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -29.2     |\n",
      "|    reward             | 0.331651  |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 11.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 254        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.4      |\n",
      "|    explained_variance | -0.0108    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 40         |\n",
      "|    reward             | -4.2731113 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 32         |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 256      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -11.4    |\n",
      "|    explained_variance | -0.106   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 42.1     |\n",
      "|    reward             | 3.527686 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 15.7     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 257       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | 0.0201    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 57.9      |\n",
      "|    reward             | 6.7649264 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 24.8      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 256          |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 31           |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -11.4        |\n",
      "|    explained_variance | -0.0221      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | 32.2         |\n",
      "|    reward             | -0.049489442 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 12.1         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 250       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | 0.00345   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -102      |\n",
      "|    reward             | 12.343373 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 141       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 252       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | 0.00565   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 3.91      |\n",
      "|    reward             | 2.9768488 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.31      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 253       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | -0.0014   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -11.2     |\n",
      "|    reward             | 1.3067911 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.62      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 254        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.4      |\n",
      "|    explained_variance | 0.0424     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -9.15      |\n",
      "|    reward             | -0.5823212 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.88       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 255       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 7.44      |\n",
      "|    reward             | 3.655028  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.8       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 256        |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.4      |\n",
      "|    explained_variance | -0.0195    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | -6.43      |\n",
      "|    reward             | -13.327685 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 42         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 257       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -1.24e+03 |\n",
      "|    reward             | 15.952341 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.42e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 257        |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.4      |\n",
      "|    explained_variance | 9.98e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 43.3       |\n",
      "|    reward             | 0.37031364 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 28.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | 0.0113    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -38       |\n",
      "|    reward             | 1.9715408 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 13.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | -0.00612  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 27.3      |\n",
      "|    reward             | 3.6931682 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.41      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 259        |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.5      |\n",
      "|    explained_variance | 0.0149     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | 9.25       |\n",
      "|    reward             | -1.7333504 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.62       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 259       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | 0.134     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 144       |\n",
      "|    reward             | 5.7239776 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 163       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -10.1     |\n",
      "|    reward             | 0.7372443 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.871     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | -0.012    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | 11.7      |\n",
      "|    reward             | 0.9154652 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.27      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 260        |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.5      |\n",
      "|    explained_variance | -0.709     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -17.9      |\n",
      "|    reward             | 0.74288523 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.08       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 261        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.5      |\n",
      "|    explained_variance | -0.0836    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -15.6      |\n",
      "|    reward             | -1.3653257 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 61.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | -0.024    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 86.4      |\n",
      "|    reward             | -2.835574 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 82.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | -0.00183  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 0.919     |\n",
      "|    reward             | 30.562641 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 38.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 61         |\n",
      "|    reward             | 0.29656655 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 30.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 68        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | 0.0135    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -36       |\n",
      "|    reward             | 1.1817856 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 12.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.5      |\n",
      "|    explained_variance | 0.146      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | 69.3       |\n",
      "|    reward             | -1.0236704 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 30.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | 0.0118    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | -2.57     |\n",
      "|    reward             | 1.1907343 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | -0.00168  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -61.4     |\n",
      "|    reward             | 2.8145509 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 42.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | 0.0181    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | 41.6      |\n",
      "|    reward             | 5.2570167 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 30.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 78         |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.5      |\n",
      "|    explained_variance | -0.014     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -7.25      |\n",
      "|    reward             | -0.2503744 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.88       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 261         |\n",
      "|    iterations         | 4200        |\n",
      "|    time_elapsed       | 80          |\n",
      "|    total_timesteps    | 21000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -11.5       |\n",
      "|    explained_variance | -0.00215    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4199        |\n",
      "|    policy_loss        | -98.3       |\n",
      "|    reward             | -0.59463835 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 86.6        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 261      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 82       |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -11.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -23.2    |\n",
      "|    reward             | 2.902168 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 5.87     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | 27.4      |\n",
      "|    reward             | 1.9206802 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 33.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 260      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 86       |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -11.5    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 108      |\n",
      "|    reward             | 3.270264 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 98.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 260      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -11.5    |\n",
      "|    explained_variance | 0.00455  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | 0.786    |\n",
      "|    reward             | 8.13564  |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 99.6     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 260         |\n",
      "|    iterations         | 4700        |\n",
      "|    time_elapsed       | 90          |\n",
      "|    total_timesteps    | 23500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -11.5       |\n",
      "|    explained_variance | 0.000393    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4699        |\n",
      "|    policy_loss        | -9.77       |\n",
      "|    reward             | -0.12156626 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 6.62        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | -0.00358  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | -112      |\n",
      "|    reward             | -2.016906 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 149       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | -0.0124   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 13.6      |\n",
      "|    reward             | 2.7948086 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 7.64      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 260      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 96       |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -11.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -59.8    |\n",
      "|    reward             | 1.471569 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 40.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | -1.07e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | 174       |\n",
      "|    reward             | 1.2861205 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 417       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 260      |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 99       |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -11.5    |\n",
      "|    explained_variance | 0.000398 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 74.4     |\n",
      "|    reward             | 27.18916 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 282      |\n",
      "------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14084131.34\n",
      "total_reward: 13084131.34\n",
      "total_cost: 47882.77\n",
      "total_trades: 17648\n",
      "Sharpe: 1.204\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 260        |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.6      |\n",
      "|    explained_variance | -0.000444  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | -22.8      |\n",
      "|    reward             | 0.77378356 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 4.86       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 260        |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 103        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.6      |\n",
      "|    explained_variance | 0.0195     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -31.7      |\n",
      "|    reward             | 0.95309997 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 16.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 260        |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 105        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.6      |\n",
      "|    explained_variance | -0.00322   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | -21.8      |\n",
      "|    reward             | -4.0434084 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 48.7       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 260         |\n",
      "|    iterations         | 5600        |\n",
      "|    time_elapsed       | 107         |\n",
      "|    total_timesteps    | 28000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -11.6       |\n",
      "|    explained_variance | -0.0287     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5599        |\n",
      "|    policy_loss        | -6.6        |\n",
      "|    reward             | -0.29433775 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 9.32        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 260        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.7      |\n",
      "|    explained_variance | 0.0152     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -439       |\n",
      "|    reward             | -16.975143 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.19e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 260        |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 111        |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | -9.52      |\n",
      "|    reward             | 0.14098172 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.81       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.7     |\n",
      "|    explained_variance | -0.00154  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -1.17     |\n",
      "|    reward             | 1.0726352 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.53      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.7     |\n",
      "|    explained_variance | 0.000958  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | 16.5      |\n",
      "|    reward             | -3.396383 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 4.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.7     |\n",
      "|    explained_variance | 0.00362   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | -9.73     |\n",
      "|    reward             | -5.365951 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 10.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 260        |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 118        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.7      |\n",
      "|    explained_variance | -0.000965  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | 77.2       |\n",
      "|    reward             | -1.5659945 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 43         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.7     |\n",
      "|    explained_variance | 9.87e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 127       |\n",
      "|    reward             | 24.177277 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 286       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.7     |\n",
      "|    explained_variance | 0.00087   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 35.5      |\n",
      "|    reward             | 2.9200242 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 11.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 261        |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 124        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.7      |\n",
      "|    explained_variance | -0.000734  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | -23        |\n",
      "|    reward             | -3.6106138 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 6.61       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 261        |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 126        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.7      |\n",
      "|    explained_variance | 0.000314   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -25.4      |\n",
      "|    reward             | 0.63201827 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 10.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 127       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.7     |\n",
      "|    explained_variance | -0.000746 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -357      |\n",
      "|    reward             | -17.15272 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.46e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -134      |\n",
      "|    reward             | 1.1525321 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 197       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 131        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.6      |\n",
      "|    explained_variance | -0.000247  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | -117       |\n",
      "|    reward             | -1.1806153 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 243        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 133        |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | 31.7       |\n",
      "|    reward             | -0.4013448 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 6.54       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 1.18      |\n",
      "|    reward             | 2.9611917 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.63      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 137       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | -159      |\n",
      "|    reward             | 1.9534606 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 278       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 262      |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 139      |\n",
      "|    total_timesteps    | 36500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -11.7    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | -3.64    |\n",
      "|    reward             | 0.717267 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 28.7     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | 24.1      |\n",
      "|    reward             | 1.1703293 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 11.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 142        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | 98.1       |\n",
      "|    reward             | -17.865915 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 94.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 144        |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.8      |\n",
      "|    explained_variance | -1.04      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | -35.6      |\n",
      "|    reward             | 0.35013995 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 9.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 146       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.7     |\n",
      "|    explained_variance | -0.0494   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | -61.3     |\n",
      "|    reward             | 0.5039072 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 24.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 148        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 13.2       |\n",
      "|    reward             | -0.5058476 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 6.61       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 150       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.8     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | 88.8      |\n",
      "|    reward             | 3.3070047 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 78.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 152        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -45.9      |\n",
      "|    reward             | -3.0588233 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 33.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 153       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | 159       |\n",
      "|    reward             | 22.210886 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 247       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 262         |\n",
      "|    iterations         | 8200        |\n",
      "|    time_elapsed       | 155         |\n",
      "|    total_timesteps    | 41000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -11.8       |\n",
      "|    explained_variance | -3.44       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8199        |\n",
      "|    policy_loss        | -11.5       |\n",
      "|    reward             | -0.27728772 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 1.92        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 157        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.8      |\n",
      "|    explained_variance | 0.0366     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | 10         |\n",
      "|    reward             | -3.1658046 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 3.97       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | -95.4     |\n",
      "|    reward             | -2.375591 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 63.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 161       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -60.8     |\n",
      "|    reward             | 1.8667129 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 37        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 163        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | -38.9      |\n",
      "|    reward             | -21.191483 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 133        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 165       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.8     |\n",
      "|    explained_variance | -12.1     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | 120       |\n",
      "|    reward             | 1.0746595 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 151       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 167       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -10.9     |\n",
      "|    reward             | 0.1675836 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 1.19      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 169        |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.9      |\n",
      "|    explained_variance | -0.217     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | 32.1       |\n",
      "|    reward             | 0.13623548 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 10.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 171       |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.9     |\n",
      "|    explained_variance | -0.111    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | 11.9      |\n",
      "|    reward             | 0.7092128 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 5.1       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 173        |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.9      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | 44         |\n",
      "|    reward             | 0.10169871 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 19.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 174        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.9      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 73         |\n",
      "|    reward             | -1.1853088 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 39.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 9300       |\n",
      "|    time_elapsed       | 176        |\n",
      "|    total_timesteps    | 46500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.9      |\n",
      "|    explained_variance | -4.69      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9299       |\n",
      "|    policy_loss        | -10.9      |\n",
      "|    reward             | 0.34281734 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 2.26       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 262      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 178      |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | -36.4    |\n",
      "|    reward             | 1.683786 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 34.9     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 180        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.9      |\n",
      "|    explained_variance | 0.103      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | -8.21      |\n",
      "|    reward             | 0.43456307 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 182        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12        |\n",
      "|    explained_variance | 0.00998    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -24.7      |\n",
      "|    reward             | -0.6335306 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 5.74       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 262      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 184      |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -12      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | -9.57    |\n",
      "|    reward             | 6.805767 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 2.33     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 186       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 95.7      |\n",
      "|    reward             | 10.779902 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 98.4      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 262         |\n",
      "|    iterations         | 9900        |\n",
      "|    time_elapsed       | 188         |\n",
      "|    total_timesteps    | 49500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -12         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9899        |\n",
      "|    policy_loss        | -2.18       |\n",
      "|    reward             | -0.18708245 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 0.708       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 190       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 16.2      |\n",
      "|    reward             | 0.6125157 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 3.25      |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4865922.48\n",
      "total_reward: 3865922.48\n",
      "total_cost: 1647.41\n",
      "total_trades: 9180\n",
      "Sharpe: 0.711\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 60       |\n",
      "|    time_elapsed    | 190      |\n",
      "|    total_timesteps | 11572    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 116      |\n",
      "|    critic_loss     | 34.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11471    |\n",
      "|    reward          | 5.229109 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 495      |\n",
      "|    total_timesteps | 23144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 36.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23043    |\n",
      "|    reward          | 5.229109 |\n",
      "---------------------------------\n",
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5626296.62\n",
      "total_reward: 4626296.62\n",
      "total_cost: 999.00\n",
      "total_trades: 8676\n",
      "Sharpe: 0.754\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 738      |\n",
      "|    total_timesteps | 34716    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -30.4    |\n",
      "|    critic_loss     | 22.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 34615    |\n",
      "|    reward          | 5.229109 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 992      |\n",
      "|    total_timesteps | 46288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -27.9    |\n",
      "|    critic_loss     | 19.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 46187    |\n",
      "|    reward          | 5.229109 |\n",
      "---------------------------------\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 191          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 10           |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.013240621 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008418084 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | -0.00736    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.59        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00961    |\n",
      "|    reward               | 0.5676738   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.34        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 177         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009255592 |\n",
      "|    clip_fraction        | 0.0789      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.0134      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 35.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0086     |\n",
      "|    reward               | -0.3952839  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 45.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 174         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007030284 |\n",
      "|    clip_fraction        | 0.0933      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.0868      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00586    |\n",
      "|    reward               | -0.63799644 |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3304842.48\n",
      "total_reward: 2304842.48\n",
      "total_cost: 71063.00\n",
      "total_trades: 22604\n",
      "Sharpe: 0.747\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010222232 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.9         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    reward               | 2.566349    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 8.01        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008348336 |\n",
      "|    clip_fraction        | 0.0574      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.0272      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.2        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00743    |\n",
      "|    reward               | 2.2875934   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 47.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 172         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007815599 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.0291      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.5        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00528    |\n",
      "|    reward               | 1.8836776   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 70.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 171          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077115335 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | 0.0505       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.13         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00863     |\n",
      "|    reward               | -0.18471754  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 24.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011871785 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.000456    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.99        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | 0.26954967  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 30.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009900564 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | -0.00635    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21.7        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00486    |\n",
      "|    reward               | 0.11560987  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 37.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009877464 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.0181      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.56        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00402    |\n",
      "|    reward               | -5.1669416  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 35.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 139        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01308647 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -11.4      |\n",
      "|    explained_variance   | -0.0293    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 4.76       |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.00685   |\n",
      "|    reward               | 0.2538556  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 11.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010130123 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.5       |\n",
      "|    explained_variance   | 0.0242      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.1        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00473    |\n",
      "|    reward               | 0.38033915  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 22.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007545679 |\n",
      "|    clip_fraction        | 0.0749      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.5       |\n",
      "|    explained_variance   | 0.0267      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 30.1        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00655    |\n",
      "|    reward               | -2.1960201  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 60.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 161          |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092271725 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -11.6        |\n",
      "|    explained_variance   | -0.0414      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 20.7         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00747     |\n",
      "|    reward               | 4.3613353    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 26.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014148386 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.6       |\n",
      "|    explained_variance   | -0.0031     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00621    |\n",
      "|    reward               | -0.35241175 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 38.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009013707 |\n",
      "|    clip_fraction        | 0.0918      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.6       |\n",
      "|    explained_variance   | -0.0101     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 44.3        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00758    |\n",
      "|    reward               | 0.15244891  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 80.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011095291 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.6       |\n",
      "|    explained_variance   | -0.00249    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 43.2        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00696    |\n",
      "|    reward               | -0.23023285 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 68.4        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2501911.41\n",
      "total_reward: 1501911.41\n",
      "total_cost: 68778.11\n",
      "total_trades: 22164\n",
      "Sharpe: 0.547\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 199         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011827391 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.6       |\n",
      "|    explained_variance   | -0.0124     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.06        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00812    |\n",
      "|    reward               | -0.9152955  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 10.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010873556 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.7       |\n",
      "|    explained_variance   | 0.0172      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.1        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    reward               | 0.094247796 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 29.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 224         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008116199 |\n",
      "|    clip_fraction        | 0.0854      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.7       |\n",
      "|    explained_variance   | 0.0136      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 50.2        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00718    |\n",
      "|    reward               | -9.29403    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 83          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 236         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011994912 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.7       |\n",
      "|    explained_variance   | 0.0173      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.6        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    reward               | 1.5243677   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 32.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 248         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011427312 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.7       |\n",
      "|    explained_variance   | 0.0783      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.73        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    reward               | 0.2779133   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 47.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 260         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012805929 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.7       |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 31.8        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    reward               | 3.1152675   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 54          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012191557 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.7       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 30.9        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    reward               | -0.40883943 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 34.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 52        |\n",
      "|    time_elapsed    | 221       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 925       |\n",
      "|    critic_loss     | 318       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 11471     |\n",
      "|    reward          | 3.6894298 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3733721.75\n",
      "total_reward: 2733721.75\n",
      "total_cost: 999.00\n",
      "total_trades: 2892\n",
      "Sharpe: 0.550\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 53        |\n",
      "|    time_elapsed    | 430       |\n",
      "|    total_timesteps | 23144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 779       |\n",
      "|    critic_loss     | 365       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 23043     |\n",
      "|    reward          | 3.6894298 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 51        |\n",
      "|    time_elapsed    | 677       |\n",
      "|    total_timesteps | 34716     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 355       |\n",
      "|    critic_loss     | 98.9      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 34615     |\n",
      "|    reward          | 3.6894298 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3733721.75\n",
      "total_reward: 2733721.75\n",
      "total_cost: 999.00\n",
      "total_trades: 2892\n",
      "Sharpe: 0.550\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 47        |\n",
      "|    time_elapsed    | 966       |\n",
      "|    total_timesteps | 46288     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 373       |\n",
      "|    critic_loss     | 75.2      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 46187     |\n",
      "|    reward          | 3.6894298 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 51       |\n",
      "|    time_elapsed    | 224      |\n",
      "|    total_timesteps | 11572    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.01e+03 |\n",
      "|    critic_loss     | 163      |\n",
      "|    ent_coef        | 0.305    |\n",
      "|    ent_coef_loss   | 116      |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 11471    |\n",
      "|    reward          | 6.400812 |\n",
      "---------------------------------\n",
      "day: 2892, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4377797.90\n",
      "total_reward: 3377797.90\n",
      "total_cost: 999.00\n",
      "total_trades: 8676\n",
      "Sharpe: 0.771\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 45       |\n",
      "|    time_elapsed    | 504      |\n",
      "|    total_timesteps | 23144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.48e+03 |\n",
      "|    critic_loss     | 95.7     |\n",
      "|    ent_coef        | 0.972    |\n",
      "|    ent_coef_loss   | 2.81     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 23043    |\n",
      "|    reward          | 6.400812 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 744      |\n",
      "|    total_timesteps | 34716    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.13e+04 |\n",
      "|    critic_loss     | 1.25e+03 |\n",
      "|    ent_coef        | 3.09     |\n",
      "|    ent_coef_loss   | -110     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 34615    |\n",
      "|    reward          | 6.400812 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 43       |\n",
      "|    time_elapsed    | 1064     |\n",
      "|    total_timesteps | 46288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.56e+04 |\n",
      "|    critic_loss     | 3.7e+04  |\n",
      "|    ent_coef        | 9.83     |\n",
      "|    ent_coef_loss   | -223     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 46187    |\n",
      "|    reward          | 6.400812 |\n",
      "---------------------------------\n",
      "day: 2892, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4377797.90\n",
      "total_reward: 3377797.90\n",
      "total_cost: 999.00\n",
      "total_trades: 8676\n",
      "Sharpe: 0.771\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c,\n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None\n",
    "\n",
    "trained_ddpg = agent.train_model(model=model_ddpg,\n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None\n",
    "\n",
    "trained_ppo = agent.train_model(model=model_ppo,\n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=50000) if if_using_ppo else None\n",
    "\n",
    "trained_td3 = agent.train_model(model=model_td3,\n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None\n",
    "\n",
    "trained_sac = agent.train_model(model=model_sac,\n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=50000) if if_using_sac else None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 58        |\n",
      "|    time_elapsed    | 198       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 219       |\n",
      "|    critic_loss     | 139       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 63626     |\n",
      "|    reward          | 3.6894298 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3733721.75\n",
      "total_reward: 2733721.75\n",
      "total_cost: 999.00\n",
      "total_trades: 2892\n",
      "Sharpe: 0.550\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 58        |\n",
      "|    time_elapsed    | 393       |\n",
      "|    total_timesteps | 23144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 152       |\n",
      "|    critic_loss     | 314       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 75198     |\n",
      "|    reward          | 3.6894298 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 59        |\n",
      "|    time_elapsed    | 584       |\n",
      "|    total_timesteps | 34716     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 123       |\n",
      "|    critic_loss     | 31.9      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 86770     |\n",
      "|    reward          | 3.6894298 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 58        |\n",
      "|    time_elapsed    | 793       |\n",
      "|    total_timesteps | 46288     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 77.4      |\n",
      "|    critic_loss     | 31        |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 98342     |\n",
      "|    reward          | 3.6894298 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 48       |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 11572    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.64e+05 |\n",
      "|    critic_loss     | 2.39e+05 |\n",
      "|    ent_coef        | 44.9     |\n",
      "|    ent_coef_loss   | -371     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 61371    |\n",
      "|    reward          | 6.400812 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 50       |\n",
      "|    time_elapsed    | 455      |\n",
      "|    total_timesteps | 23144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22e+05 |\n",
      "|    critic_loss     | 3.13e+06 |\n",
      "|    ent_coef        | 143      |\n",
      "|    ent_coef_loss   | -486     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 72943    |\n",
      "|    reward          | 6.400812 |\n",
      "---------------------------------\n",
      "day: 2892, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4377797.90\n",
      "total_reward: 3377797.90\n",
      "total_cost: 999.00\n",
      "total_trades: 8676\n",
      "Sharpe: 0.771\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 48       |\n",
      "|    time_elapsed    | 710      |\n",
      "|    total_timesteps | 34716    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.65e+06 |\n",
      "|    critic_loss     | 2.17e+07 |\n",
      "|    ent_coef        | 455      |\n",
      "|    ent_coef_loss   | -600     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 84515    |\n",
      "|    reward          | 6.400812 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 49       |\n",
      "|    time_elapsed    | 926      |\n",
      "|    total_timesteps | 46288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.2e+06  |\n",
      "|    critic_loss     | 2.61e+08 |\n",
      "|    ent_coef        | 1.45e+03 |\n",
      "|    ent_coef_loss   | -711     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 96087    |\n",
      "|    reward          | 6.400812 |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3,\n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None\n",
    "\n",
    "trained_sac = agent.train_model(model=model_sac,\n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=50000) if if_using_sac else None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.td3.td3.TD3'>\n",
      "<class 'stable_baselines3.sac.sac.SAC'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trained_td3))\n",
    "print(type(trained_sac))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.td3.td3.TD3'>\n",
      "<class 'stable_baselines3.sac.sac.SAC'>\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "trained_a2c.save(data_path + \"agents/agent_a2c\") if if_using_a2c else None\n",
    "trained_ddpg.save(data_path + \"agents/agent_ddpg\") if if_using_ddpg else None\n",
    "trained_ppo.save(data_path + \"agents/agent_ppo\") if if_using_ppo else None\n",
    "trained_td3.save(data_path + \"agents/agent_td3\") #if if_using_td3 else None\n",
    "trained_sac.save(data_path + \"agents/agent_sac\") #if if_using_sac else None\n",
    "print(type(trained_td3))\n",
    "print(type(trained_sac))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "######################################################################\n",
    "##############################   SECTION3   ##########################\n",
    "######################################################################\n",
    "\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']\n",
    "trade = trade.set_index(trade.columns[0])\n",
    "trade.index.names = ['']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
    "\n",
    "trained_a2c = A2C.load(data_path + 'agents/agent_a2c') if if_using_a2c else None\n",
    "trained_ddpg = DDPG.load(data_path + \"agents/agent_ddpg\") if if_using_ddpg else None\n",
    "trained_ppo = PPO.load(data_path + \"agents/agent_ppo\") if if_using_ppo else None\n",
    "trained_td3 = TD3.load(data_path + \"agents/agent_td3\") if if_using_td3 else None\n",
    "trained_sac = SAC.load(data_path + \"agents/agent_sac\") if if_using_sac else None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 8, State Space: 81\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(trade.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "# print(num_stock_shares)\n",
    "# print(buy_cost_list)\n",
    "# print(sell_cost_list)\n",
    "# print(state_space)\n",
    "# print(stock_dimension)\n",
    "# print(INDICATORS)\n",
    "# print(stock_dimension)\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date   tic        open        high         low       close  \\\n",
      "0    2020-07-01  aapl   91.279999   91.839996   90.977501   89.133003   \n",
      "0    2020-07-01  amzn  137.899506  144.750000  137.699997  143.934998   \n",
      "0    2020-07-01   cat  129.380005  129.399994  125.879997  116.095177   \n",
      "0    2020-07-01    hd  249.649994  250.330002  246.929993  228.232849   \n",
      "0    2020-07-01   ibm  114.980881  115.898659  113.164436   95.288437   \n",
      "..          ...   ...         ...         ...         ...         ...   \n",
      "608  2022-11-29    hd  316.000000  320.000000  315.619995  305.557312   \n",
      "608  2022-11-29   ibm  145.910004  147.169998  145.699997  139.558807   \n",
      "608  2022-11-29  intc   28.850000   29.180000   28.740000   28.240854   \n",
      "608  2022-11-29  msft  241.399994  242.789993  238.210007  238.217743   \n",
      "608  2022-11-29     t   18.790001   19.030001   18.750000   17.527452   \n",
      "\n",
      "          volume  day      macd     boll_ub     boll_lb     rsi_30  \\\n",
      "0    110737200.0  2.0  2.988731   91.903729   79.491807  62.807139   \n",
      "0    127268000.0  2.0  4.503504  142.937010  121.837240  67.910978   \n",
      "0      2807800.0  2.0  1.224568  125.691191  109.072290  52.865425   \n",
      "0      3677300.0  2.0  2.187403  237.721138  219.254060  55.509700   \n",
      "0      4869967.0  2.0 -1.017657  108.099840   90.300100  47.489717   \n",
      "..           ...  ...       ...         ...         ...        ...   \n",
      "608    3505600.0  1.0  8.491188  324.439971  268.177372  56.444578   \n",
      "608    2754700.0  1.0  4.523366  145.729322  125.716428  63.696291   \n",
      "608   24361500.0  1.0  0.435235   30.506556   26.224579  49.010665   \n",
      "608   17956300.0  1.0  2.267116  254.737121  213.164572  49.156757   \n",
      "608   24088000.0  1.0  0.446189   17.881416   16.719863  60.467981   \n",
      "\n",
      "         cci_30      dx_30  close_30_sma  close_60_sma        vix  turbulence  \n",
      "0    107.505457  29.730532     83.213407     77.050533  28.620001   14.645263  \n",
      "0    160.233497  52.645714    129.056634    122.777067  28.620001   14.645263  \n",
      "0     35.797095  14.457404    114.903152    109.354755  28.620001   14.645263  \n",
      "0     20.613726   8.254306    227.201665    212.881532  28.620001   14.645263  \n",
      "0    -74.271519   5.476161     99.240162     97.829753  28.620001   14.645263  \n",
      "..          ...        ...           ...           ...        ...         ...  \n",
      "608   83.144966  22.103961    289.574117    280.954247  21.889999    3.949013  \n",
      "608   83.912091  42.080709    131.775917    124.174526  21.889999    3.949013  \n",
      "608   33.364885   0.768171     27.603657     27.243324  21.889999    3.949013  \n",
      "608   34.946232   0.428685    234.291056    237.173987  21.889999    3.949013  \n",
      "608   64.832371  20.258599     16.852541     15.702813  21.889999    3.949013  \n",
      "\n",
      "[4872 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "train = data_split(processed_full, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(trade)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "\n",
    "e_trade_gym = StockTradingEnv(df = trade, turbulence_threshold = 70,risk_indicator_col='vix', **env_kwargs)\n",
    "# env_trade, obs_trade = e_trade_gym.get_sb_env()e_trade_gym = StockTradingEnv(df = trade, **env_kwargs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n",
      "hit end!\n",
      "hit end!\n",
      "hit end!\n",
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
    "    model=trained_a2c,\n",
    "    environment = e_trade_gym) if if_using_a2c else (None, None)\n",
    "\n",
    "df_account_value_ddpg, df_actions_ddpg= DRLAgent.DRL_prediction(\n",
    "    model=trained_ddpg,\n",
    "    environment = e_trade_gym) if if_using_ddpg else (None, None)\n",
    "\n",
    "df_account_value_ppo, df_actions_ppo = DRLAgent.DRL_prediction(\n",
    "    model=trained_ppo,\n",
    "    environment = e_trade_gym) if if_using_ppo else (None, None)\n",
    "\n",
    "df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
    "    model=trained_td3,\n",
    "    environment = e_trade_gym) if if_using_td3 else (None, None)\n",
    "\n",
    "df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
    "    model=trained_sac,\n",
    "    environment = e_trade_gym) if if_using_sac else (None, None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "# Mean Variance Optimization\n",
    "\n",
    "# Helps us process data into a form for weight calculation\n",
    "def process_df_for_mvo(df):\n",
    "  df = df.sort_values(['date','tic'],ignore_index=True)[['date','tic','close']]\n",
    "  fst = df\n",
    "  fst = fst.iloc[0:stock_dimension, :]\n",
    "  tic = fst['tic'].tolist()\n",
    "\n",
    "  mvo = pd.DataFrame()\n",
    "\n",
    "  for k in range(len(tic)):\n",
    "    mvo[tic[k]] = 0\n",
    "\n",
    "  for i in range(df.shape[0]//stock_dimension):\n",
    "    n = df\n",
    "    n = n.iloc[i * stock_dimension:(i+1) * stock_dimension, :]\n",
    "    date = n['date'][i*stock_dimension]\n",
    "    mvo.loc[date] = n['close'].tolist()\n",
    "\n",
    "  return mvo\n",
    "\n",
    "# Calculates weights of average return and covariance matrix\n",
    "def StockReturnsComputing(StockPrice, Rows, Columns):\n",
    "  import numpy as np\n",
    "  StockReturn = np.zeros([Rows-1, Columns])\n",
    "  for j in range(Columns):        # j: Assets\n",
    "    for i in range(Rows-1):     # i: Daily Prices\n",
    "      StockReturn[i,j]=((StockPrice[i+1, j]-StockPrice[i,j])/StockPrice[i,j])* 100\n",
    "\n",
    "  return StockReturn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 89.133, 143.935, 116.095, ...,  53.128, 198.346,  16.805],\n       [ 89.133, 144.515, 117.624, ...,  53.417, 199.858,  16.906],\n       [ 91.517, 152.852, 119.199, ...,  53.787, 204.16 ,  17.136],\n       ...,\n       [147.287,  93.41 , 229.906, ...,  28.671, 245.315,  17.629],\n       [143.418,  93.95 , 226.59 , ...,  28.075, 239.635,  17.352],\n       [140.385,  92.42 , 229.341, ...,  28.241, 238.218,  17.527]])"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StockData = process_df_for_mvo(train)\n",
    "TradeData = process_df_for_mvo(trade)\n",
    "\n",
    "TradeData.to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean returns of assets in k-portfolio 1\n",
      " [0.136 0.158 0.066 0.103 0.033 0.076 0.103 0.038]\n",
      "Variance-Covariance matrix of returns\n",
      " [[3.156 1.592 1.722 1.303 1.218 1.674 1.561 0.876]\n",
      " [1.592 4.533 1.657 1.299 1.158 1.516 1.808 0.761]\n",
      " [1.722 1.657 4.019 1.577 1.513 1.929 1.711 1.187]\n",
      " [1.303 1.299 1.577 2.373 1.166 1.501 1.366 0.953]\n",
      " [1.218 1.158 1.513 1.166 2.052 1.399 1.296 0.944]\n",
      " [1.674 1.516 1.929 1.501 1.399 3.289 1.9   1.034]\n",
      " [1.561 1.808 1.711 1.366 1.296 1.9   2.759 0.916]\n",
      " [0.876 0.761 1.187 0.953 0.944 1.034 0.916 1.577]]\n"
     ]
    }
   ],
   "source": [
    "#compute asset returns\n",
    "arStockPrices = np.asarray(StockData)\n",
    "[Rows, Cols]=arStockPrices.shape\n",
    "arReturns = StockReturnsComputing(arStockPrices, Rows, Cols)\n",
    "\n",
    "#compute mean returns and variance covariance matrix of returns\n",
    "meanReturns = np.mean(arReturns, axis = 0)\n",
    "covReturns = np.cov(arReturns, rowvar=False)\n",
    "\n",
    "#set precision for printing results\n",
    "np.set_printoptions(precision=3, suppress = True)\n",
    "\n",
    "#display mean returns and variance-covariance matrix of returns\n",
    "print('Mean returns of assets in k-portfolio 1\\n', meanReturns)\n",
    "print('Variance-Covariance matrix of returns\\n', covReturns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "array([412090., 358860.,      0., 229050.,      0.,      0.,      0.,\n            0.])"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "\n",
    "ef_mean = EfficientFrontier(meanReturns, covReturns, weight_bounds=(0, 0.5))\n",
    "raw_weights_mean = ef_mean.max_sharpe()\n",
    "cleaned_weights_mean = ef_mean.clean_weights()\n",
    "mvo_weights = np.array([1000000 * cleaned_weights_mean[i] for i in range(8)])\n",
    "\n",
    "mvo_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "array([4614.571, 2601.547,    0.   ,  994.126,    0.   ,    0.   ,\n          0.   ,    0.   ])"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LastPrice = np.array([1/p for p in StockData.tail(1).to_numpy()[0]])\n",
    "Initial_Portfolio = np.multiply(mvo_weights, LastPrice)\n",
    "\n",
    "Initial_Portfolio"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "                Mean Var\n2020-07-01  1.012656e+06\n2020-07-02  1.014485e+06\n2020-07-06  1.048137e+06\n2020-07-07  1.037411e+06\n2020-07-08  1.059415e+06\n...                  ...\n2022-11-22  1.243966e+06\n2022-11-23  1.247230e+06\n2022-11-25  1.236456e+06\n2022-11-28  1.212838e+06\n2022-11-29  1.192015e+06\n\n[609 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mean Var</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-07-01</th>\n      <td>1.012656e+06</td>\n    </tr>\n    <tr>\n      <th>2020-07-02</th>\n      <td>1.014485e+06</td>\n    </tr>\n    <tr>\n      <th>2020-07-06</th>\n      <td>1.048137e+06</td>\n    </tr>\n    <tr>\n      <th>2020-07-07</th>\n      <td>1.037411e+06</td>\n    </tr>\n    <tr>\n      <th>2020-07-08</th>\n      <td>1.059415e+06</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-11-22</th>\n      <td>1.243966e+06</td>\n    </tr>\n    <tr>\n      <th>2022-11-23</th>\n      <td>1.247230e+06</td>\n    </tr>\n    <tr>\n      <th>2022-11-25</th>\n      <td>1.236456e+06</td>\n    </tr>\n    <tr>\n      <th>2022-11-28</th>\n      <td>1.212838e+06</td>\n    </tr>\n    <tr>\n      <th>2022-11-29</th>\n      <td>1.192015e+06</td>\n    </tr>\n  </tbody>\n</table>\n<p>609 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Portfolio_Assets = TradeData @ Initial_Portfolio\n",
    "MVO_result = pd.DataFrame(Portfolio_Assets, columns=[\"Mean Var\"])\n",
    "\n",
    "MVO_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Desktop\\Finance\\RL\\venv\\lib\\site-packages\\yfinance\\utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (610, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DOW JONES Index as Another Baseline\n",
    "\n",
    "TRAIN_START_DATE = '2009-01-01'\n",
    "TRAIN_END_DATE = '2020-07-01'\n",
    "TRADE_START_DATE = '2020-07-01'\n",
    "TRADE_END_DATE = '2022-12-01'\n",
    "# TRADE_END_DATE = '2023-05-01'\n",
    "\n",
    "df_dji = YahooDownloader(start_date = TRADE_START_DATE,\n",
    "                     end_date = TRADE_END_DATE,\n",
    "                     ticker_list = ['^dji']).fetch_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "df_dji = df_dji[['date','close']]\n",
    "fst_day = df_dji['close'][0]\n",
    "dji = pd.merge(df_dji['date'], df_dji['close'].div(fst_day).mul(1000000),\n",
    "               how='outer', left_index=True, right_index=True).set_index('date')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "df_result_a2c = df_account_value_a2c.set_index(df_account_value_a2c.columns[0]) if if_using_a2c else None\n",
    "df_result_ddpg = df_account_value_ddpg.set_index(df_account_value_ddpg.columns[0]) if if_using_ddpg else None\n",
    "df_result_ppo = df_account_value_ppo.set_index(df_account_value_ppo.columns[0]) if if_using_ppo else None\n",
    "df_result_td3 = df_account_value_td3.set_index(df_account_value_td3.columns[0]) if if_using_td3 else None\n",
    "df_result_sac = df_account_value_sac.set_index(df_account_value_sac.columns[0]) if if_using_sac else None\n",
    "\n",
    "result = pd.DataFrame()\n",
    "if if_using_a2c: result = pd.merge(result, df_result_a2c, how='outer', left_index=True, right_index=True)\n",
    "# if if_using_ddpg: result = pd.merge(result, df_result_ddpg, how='outer', left_index=True, right_index=True)\n",
    "if if_using_ppo: result = pd.merge(result, df_result_ppo, how='outer', left_index=True, right_index=True)\n",
    "# if if_using_td3: result = pd.merge(result, df_result_td3, how='outer', left_index=True, right_index=True)\n",
    "if if_using_sac: result = pd.merge(result, df_result_sac, how='outer', left_index=True, right_index=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "# Merge dataframes and view results\n",
    "\n",
    "col_name = []\n",
    "col_name.append('A2C') if if_using_a2c else None\n",
    "# col_name.append('DDPG') if if_using_ddpg else None\n",
    "col_name.append('PPO') if if_using_ppo else None\n",
    "# col_name.append('TD3') if if_using_td3 else None\n",
    "col_name.append('SAC') if if_using_sac else None\n",
    "\n",
    "result.columns = col_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_17972\\3181995248.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  result = pd.merge(result, dji, how='outer', left_index=True, right_index=True).fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "result = pd.merge(result, MVO_result, how='outer', left_index=True, right_index=True)\n",
    "result = pd.merge(result, dji, how='outer', left_index=True, right_index=True).fillna(method='bfill')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "data": {
      "text/plain": "                     A2C           PPO           SAC      Mean Var  \\\ndate                                                                 \n2020-07-01  1.000000e+06  1.000000e+06  1.000000e+06  1.012656e+06   \n2020-07-02  1.000305e+06  1.000012e+06  1.000102e+06  1.014485e+06   \n2020-07-06  1.002702e+06  1.000231e+06  1.000385e+06  1.048137e+06   \n2020-07-07  9.997840e+05  1.000074e+06  9.990859e+05  1.037411e+06   \n2020-07-08  1.002826e+06  1.000385e+06  9.997949e+05  1.059415e+06   \n...                  ...           ...           ...           ...   \n2022-11-23  1.236618e+06  1.103535e+06  1.320976e+06  1.247230e+06   \n2022-11-25  1.228017e+06  1.093084e+06  1.333216e+06  1.236456e+06   \n2022-11-28  1.209760e+06  1.075388e+06  1.306336e+06  1.212838e+06   \n2022-11-29  1.203575e+06  1.066478e+06  1.299648e+06  1.192015e+06   \n2022-11-30           NaN           NaN           NaN           NaN   \n\n                   close  \ndate                      \n2020-07-01  1.000000e+06  \n2020-07-02  1.003590e+06  \n2020-07-06  1.021452e+06  \n2020-07-07  1.006031e+06  \n2020-07-08  1.012913e+06  \n...                  ...  \n2022-11-23  1.328700e+06  \n2022-11-25  1.334644e+06  \n2022-11-28  1.315310e+06  \n2022-11-29  1.315429e+06  \n2022-11-30  1.344077e+06  \n\n[610 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A2C</th>\n      <th>PPO</th>\n      <th>SAC</th>\n      <th>Mean Var</th>\n      <th>close</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-07-01</th>\n      <td>1.000000e+06</td>\n      <td>1.000000e+06</td>\n      <td>1.000000e+06</td>\n      <td>1.012656e+06</td>\n      <td>1.000000e+06</td>\n    </tr>\n    <tr>\n      <th>2020-07-02</th>\n      <td>1.000305e+06</td>\n      <td>1.000012e+06</td>\n      <td>1.000102e+06</td>\n      <td>1.014485e+06</td>\n      <td>1.003590e+06</td>\n    </tr>\n    <tr>\n      <th>2020-07-06</th>\n      <td>1.002702e+06</td>\n      <td>1.000231e+06</td>\n      <td>1.000385e+06</td>\n      <td>1.048137e+06</td>\n      <td>1.021452e+06</td>\n    </tr>\n    <tr>\n      <th>2020-07-07</th>\n      <td>9.997840e+05</td>\n      <td>1.000074e+06</td>\n      <td>9.990859e+05</td>\n      <td>1.037411e+06</td>\n      <td>1.006031e+06</td>\n    </tr>\n    <tr>\n      <th>2020-07-08</th>\n      <td>1.002826e+06</td>\n      <td>1.000385e+06</td>\n      <td>9.997949e+05</td>\n      <td>1.059415e+06</td>\n      <td>1.012913e+06</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-11-23</th>\n      <td>1.236618e+06</td>\n      <td>1.103535e+06</td>\n      <td>1.320976e+06</td>\n      <td>1.247230e+06</td>\n      <td>1.328700e+06</td>\n    </tr>\n    <tr>\n      <th>2022-11-25</th>\n      <td>1.228017e+06</td>\n      <td>1.093084e+06</td>\n      <td>1.333216e+06</td>\n      <td>1.236456e+06</td>\n      <td>1.334644e+06</td>\n    </tr>\n    <tr>\n      <th>2022-11-28</th>\n      <td>1.209760e+06</td>\n      <td>1.075388e+06</td>\n      <td>1.306336e+06</td>\n      <td>1.212838e+06</td>\n      <td>1.315310e+06</td>\n    </tr>\n    <tr>\n      <th>2022-11-29</th>\n      <td>1.203575e+06</td>\n      <td>1.066478e+06</td>\n      <td>1.299648e+06</td>\n      <td>1.192015e+06</td>\n      <td>1.315429e+06</td>\n    </tr>\n    <tr>\n      <th>2022-11-30</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.344077e+06</td>\n    </tr>\n  </tbody>\n</table>\n<p>610 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_17972\\2224680192.py:15: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot results\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plots_path = '../plots'\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "\n",
    "plt.title('Portfolio Value Comparison')\n",
    "plt.ylabel('Portfolio Value')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(loc='upper left')\n",
    "# Save the plot to a file (e.g., PNG)\n",
    "plt.savefig(os.path.join(plots_path, 'portfolio_value_comparison_second.png'))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
